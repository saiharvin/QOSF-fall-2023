{"cells":[{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T06:17:01.048497Z","iopub.status.busy":"2023-12-09T06:17:01.047048Z","iopub.status.idle":"2023-12-09T06:17:02.810695Z","shell.execute_reply":"2023-12-09T06:17:02.809302Z","shell.execute_reply.started":"2023-12-09T06:17:01.048427Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import torch.nn as nn\n","import torch\n","\n","from qiskit  import Aer, QuantumCircuit\n","from qiskit.utils import QuantumInstance\n","from qiskit.opflow import AerPauliExpectation\n","from qiskit.circuit import Parameter\n","from qiskit.circuit.library import RealAmplitudes, ZZFeatureMap\n","from qiskit.primitives import Sampler\n","# from qiskit_algorithms.optimizers import COBYLA, L_BFGS_B\n","# from qiskit_algorithms.utils import algorithm_globals\n","\n","import time\n","\n","# from qiskit_machine_learning.algorithms.classifiers import NeuralNetworkClassifier, VQC\n","# from qiskit_machine_learning.algorithms.regressors import NeuralNetworkRegressor, VQR\n","from qiskit_machine_learning.circuit.library import QNNCircuit\n","from qiskit_machine_learning.connectors import TorchConnector\n","from qiskit_machine_learning.neural_networks import SamplerQNN, EstimatorQNN\n","# from qiskit_machine_learning.neural_networks import TwoLayerQNN\n","\n","from matplotlib import pyplot as plt\n","from IPython.display import clear_output"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T06:17:02.813546Z","iopub.status.busy":"2023-12-09T06:17:02.813037Z","iopub.status.idle":"2023-12-09T06:17:02.820108Z","shell.execute_reply":"2023-12-09T06:17:02.818540Z","shell.execute_reply.started":"2023-12-09T06:17:02.813513Z"},"trusted":true},"outputs":[],"source":["# Define the XOR input and target data\n","XOR_INPUT = np.array([[0, 0, 0], [0, 1, 0], [0, 0, 1], [0, 1, 1]], dtype=np.float32)\n","XOR_TARGET = np.array([[0], [1], [1], [0]], dtype=np.float32)\n","OR_INPUT = np.array([[1, 0, 0], [1, 1, 0], [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n","OR_TARGET = np.array([[0], [1], [1], [1]], dtype=np.float32)\n","AND_INPUT = np.array([[2, 0, 0], [2, 1, 0], [2, 0, 1], [2, 1, 1]], dtype=np.float32)\n","AND_TARGET = np.array([[0], [0], [0], [1]], dtype=np.float32)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# Convert the NumPy arrays to PyTorch tensors\n","inputsXOR = torch.from_numpy(XOR_INPUT).view(1, 4, 3)  # Add a batch and sequence dimension\n","targetsXOR = torch.from_numpy(XOR_TARGET).view(1, 4, 1)  # Add a batch and sequence dimension\n","inputsOR = torch.from_numpy(OR_INPUT).view(1, 4, 3)  # Add a batch and sequence dimension\n","targetsOR = torch.from_numpy(OR_TARGET).view(1, 4, 1)  # Add a batch and sequence dimension\n","inputsAND = torch.from_numpy(AND_INPUT).view(1, 4, 3)  # Add a batch and sequence dimension\n","targetsAND = torch.from_numpy(AND_TARGET).view(1, 4, 1)  # Add a batch and sequence dimension"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["class QLSTM(nn.Module):\n","    def __init__(self, \n","                input_size: int,\n","                hidden_size: int, \n","                n_qubits: int=4,\n","                n_qlayers: int=1,\n","                batch_first=True,\n","                backend='statevector_simulator'):\n","        super(QLSTM, self).__init__()\n","\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.concat_size = input_size + hidden_size\n","        self.n_qubits = n_qubits\n","        self.n_qlayers = n_qlayers\n","        self.batch_first = batch_first\n","        \n","        self.clayer_in = nn.Linear(self.concat_size, n_qubits)\n","        self.clayer_out = nn.Linear(2, 1)\n","\n","        # self.qi = QuantumInstance(Aer.get_backend('statevector_simulator'))\n","        # feature_map = ZZFeatureMap(self.n_qubits)\n","        # ansatz = RealAmplitudes(self.n_qubits, reps=self.n_qlayers)\n","\n","        # self.qnn1 = TwoLayerQNN(self.n_qubits, feature_map, ansatz, exp_val=AerPauliExpectation(), quantum_instance=self.qi)\n","        # self.qnn2 = TwoLayerQNN(self.n_qubits, feature_map, ansatz, exp_val=AerPauliExpectation(), quantum_instance=self.qi)\n","        # self.qnn3 = TwoLayerQNN(self.n_qubits, feature_map, ansatz, exp_val=AerPauliExpectation(), quantum_instance=self.qi)\n","        # self.qnn4 = TwoLayerQNN(self.n_qubits, feature_map, ansatz, exp_val=AerPauliExpectation(), quantum_instance=self.qi)\n","        parity = lambda x: \"{:b}\".format(x).count(\"1\") % 2\n","        qc = QNNCircuit(ansatz=RealAmplitudes(self.input_size, reps=1))\n","        self.qnn1 = SamplerQNN(circuit=qc, interpret=parity, output_shape=2)\n","        self.qnn2 = SamplerQNN(circuit=qc, interpret=parity, output_shape=2)\n","        self.qnn3 = SamplerQNN(circuit=qc, interpret=parity, output_shape=2)\n","        self.qnn4 = SamplerQNN(circuit=qc, interpret=parity, output_shape=2)\n","\n","        self.qlayer = {\n","            'forget': TorchConnector(self.qnn1),\n","            'input': TorchConnector(self.qnn2),\n","            'update': TorchConnector(self.qnn3),\n","            'output': TorchConnector(self.qnn4)\n","        }\n","        # print(\"self.qlayer\",self.qlayer)\n","\n","    def forward(self, x, init_states=None):\n","        # print(x)\n","        # print(init_states)\n","        if self.batch_first is True:\n","            batch_size, seq_length, features_size = x.size()\n","        else:\n","            seq_length, batch_size, features_size = x.size()\n","        # print(\"x.size()\",x.size())\n","        hidden_seq = []\n","        if init_states is None:\n","            h_t = torch.zeros(batch_size, self.hidden_size)  # hidden state (output)\n","            c_t = torch.zeros(batch_size, self.hidden_size)  # cell state\n","        else:\n","            # for now we ignore the fact that in PyTorch you can stack multiple RNNs\n","            # so we take only the first elements of the init_states tuple init_states[0][0], init_states[1][0]\n","            h_t, c_t = init_states\n","            h_t = h_t[0]\n","            c_t = c_t[0]\n","        # print(\"HT\",h_t)\n","        # print(\"CT\",c_t)\n","        for t in range(seq_length):\n","            # get features from the t-th element in seq, for all entries in the batch\n","            x_t = x[:, t, :]\n","            # print(\">>> x_t\", x_t)\n","            # print(\">>> h_t\", h_t)\n","            # Concatenate input and hidden state\n","            v_t = torch.cat((h_t, x_t), dim=1)\n","            # print(\">>> v_t\", v_t)\n","            # match qubit dimension\n","            y_t = self.clayer_in(v_t)\n","            # print(\">>> y_t\", y_t)\n","\n","            \n","            # print(\">>>\", y_t.size(), self.qlayer['forget'](y_t))\n","            # print(\">>> forget\", self.qlayer['forget'](y_t))\n","            # print(\">>> input\", self.qlayer['input'](y_t))\n","            # print(\">>> update\", self.qlayer['update'](y_t))\n","            # print(\">>> output\", self.qlayer['output'](y_t))\n","\n","            # print(\"***>>>self.clayer_out(self.qlayer['forget'](y_t))***\", self.clayer_out(self.qlayer['forget'](y_t)))\n","\n","            f_t = torch.sigmoid(self.clayer_out(self.qlayer['forget'](y_t)))  # forget block\n","            # print(\"***>>>***\", f_t.size())\n","            i_t = torch.sigmoid(self.clayer_out(self.qlayer['input'](y_t)))  # input block\n","            # print(\"***>>>***\", i_t.size())\n","            g_t = torch.tanh(self.clayer_out(self.qlayer['update'](y_t)))  # update block\n","            # print(\"***>>>***\", g_t.size())\n","            o_t = torch.sigmoid(self.clayer_out(self.qlayer['output'](y_t))) # output block\n","            # print(\"***>>>***\", o_t.size())\n","\n","            c_t = (f_t * c_t) + (i_t * g_t)\n","            h_t = o_t * torch.tanh(c_t)\n","\n","            hidden_seq.append(h_t.unsqueeze(0))\n","        hidden_seq = torch.cat(hidden_seq, dim=0)\n","        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n","        return hidden_seq, (h_t, c_t)"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["# input_dim = feature\n","class MultiTask_Network(nn.Module):\n","    def __init__(self, hidden_dim,\n","                output_dim_0, output_dim_1, output_dim_2,\n","                input_size: int, \n","                hidden_size: int, \n","                n_qubits: int=4,\n","                n_qlayers: int=1,\n","                batch_first=True,\n","                backend='statevector_simulator'):\n","        \n","        super(MultiTask_Network, self).__init__()\n","\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.concat_size = input_size + hidden_size\n","        self.n_qubits = n_qubits\n","        self.n_qlayers = n_qlayers\n","        self.batch_first = batch_first\n","\n","        self.clayer_in = nn.Linear(self.concat_size, n_qubits)\n","\n","        self.qrnn = QLSTM(input_size, hidden_size, n_qubits=n_qubits, n_qlayers=n_qlayers, backend=backend)\n","        \n","        self.clayer_out = nn.Linear(n_qubits, hidden_dim)\n","        # self.clayer_out_1 = nn.Linear(n_qubits, hidden_dim)     \n","        # self.clayer_out_2 = nn.Linear(n_qubits, hidden_dim)\n","\n","        self.final_0 = nn.Linear(hidden_dim, output_dim_0)\n","        self.final_1 = nn.Linear(hidden_dim, output_dim_1)     \n","        self.final_2 = nn.Linear(hidden_dim, output_dim_2)   \n","        \n","    def forward(self, x : torch.Tensor, task_id : int):\n","        x, _ = self.qrnn(x)\n","        x = torch.sigmoid(x)\n","        if task_id == 0:\n","            xf = self.clayer_out(x)\n","            x = self.final_0(xf)\n","        elif task_id == 1:\n","            xf = self.clayer_out(x)\n","            x = self.final_1(xf)\n","        elif task_id == 2:\n","            xf = self.clayer_out(x)\n","            x = self.final_2(xf)\n","        else:\n","            assert False, 'Bad Task ID passed'\n","            \n","        return x\n","    "]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["model = MultiTask_Network(250,\n","    output_dim_0=1, output_dim_1=1, output_dim_2=1,\n","    input_size=3, \n","    hidden_size=3, \n","    n_qubits=3,\n","    n_qlayers=3,\n","    batch_first=True,\n","    backend='statevector_simulator')\n","\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n","\n","# Training the RNN\n","epochs = 1000"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0, Loss: 1.8313915729522705\n","Epoch 100, Loss: 0.5583928823471069\n","Epoch 200, Loss: 0.5039840936660767\n","Epoch 300, Loss: 0.45399796962738037\n","Epoch 400, Loss: 0.4138486385345459\n","Epoch 500, Loss: 0.388151615858078\n","Epoch 600, Loss: 0.37287646532058716\n","Epoch 700, Loss: 0.3603503704071045\n","Epoch 800, Loss: 0.3497079610824585\n","Epoch 900, Loss: 0.34163859486579895\n"]}],"source":["losses_per_epoch = []\n","for epoch in range(epochs):\n","    # Forward pass\n","    outputsXOR = model(inputsXOR, task_id = 0)\n","    lossXOR = criterion(outputsXOR, targetsXOR)\n","    outputsOR = model(inputsOR, task_id = 1)\n","    lossOR = criterion(outputsOR, targetsOR)\n","    outputsAND = model(inputsAND, task_id = 2)\n","    lossAND = criterion(outputsAND, targetsAND)\n","\n","    loss = lossXOR + lossOR + lossAND\n","    losses_per_epoch.append(loss.item())\n","\n","    # Backward pass and optimization\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    # Print the loss every 100 epochs\n","    if epoch % 100 == 0:\n","        print(f'Epoch {epoch}, Loss: {loss.item()}')"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Input: [0. 0. 0.], Output: 0.10422071814537048, Target: 0.0\n","Input: [0. 1. 0.], Output: 0.5816405415534973, Target: 1.0\n","Input: [0. 0. 1.], Output: 0.6324537992477417, Target: 1.0\n","Input: [0. 1. 1.], Output: 0.6817087531089783, Target: 0.0\n"]}],"source":["with torch.no_grad():\n","    test_outputs = model(inputsXOR, task_id = 0)\n","    for i in range(len(XOR_INPUT)):\n","        input_data = XOR_INPUT[i]\n","        output = test_outputs[0, i, 0].item()\n","        target = XOR_TARGET[i, 0]\n","        print(f'Input: {input_data}, Output: {output}, Target: {target}')"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Input: [1. 0. 0.], Output: 0.04585497826337814, Target: 0.0\n","Input: [1. 1. 0.], Output: 0.8216104507446289, Target: 1.0\n","Input: [1. 0. 1.], Output: 0.9951233863830566, Target: 1.0\n","Input: [1. 1. 1.], Output: 1.1369589567184448, Target: 1.0\n"]}],"source":["with torch.no_grad():\n","    test_outputs = model(inputsOR, task_id = 1)\n","    for i in range(len(OR_INPUT)):\n","        input_data = OR_INPUT[i]\n","        output = test_outputs[0, i, 0].item()\n","        target = OR_TARGET[i, 0]\n","        print(f'Input: {input_data}, Output: {output}, Target: {target}')"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Input: [2. 0. 0.], Output: -0.1384308785200119, Target: 0.0\n","Input: [2. 1. 0.], Output: 0.26781827211380005, Target: 0.0\n","Input: [2. 0. 1.], Output: 0.3848787844181061, Target: 0.0\n","Input: [2. 1. 1.], Output: 0.48680245876312256, Target: 1.0\n"]}],"source":["with torch.no_grad():\n","    test_outputs = model(inputsAND, task_id = 2)\n","    for i in range(len(AND_INPUT)):\n","        input_data = AND_INPUT[i]\n","        output = test_outputs[0, i, 0].item()\n","        target = AND_TARGET[i, 0]\n","        print(f'Input: {input_data}, Output: {output}, Target: {target}')"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30615,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":4}
